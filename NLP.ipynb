{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import pandas as pd\n",
      "import matplotlib\n",
      "import math\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "font = {'family' : 'sans-serif',\n",
      "        'weight' : 'bold',\n",
      "        'size'   : 10}\n",
      "\n",
      "matplotlib.rc('font', **font)\n",
      "\n",
      "\n",
      "__location__ = os.path.realpath((os.getcwd()))\n",
      "\n",
      "raw_data = pd.read_csv(os.path.join(__location__, 'data/cleaned_data.csv'))\n",
      "raw_data['date_time']  = pd.to_datetime(raw_data['date_time'])\n",
      "\n",
      "user_messages = raw_data[raw_data['user']!='WhatsApp']\n",
      "notifications = raw_data[raw_data['user']=='WhatsApp']\n",
      "\n",
      "print 'loaded',len(user_messages),'user messages and',len(notifications),'WhatsApp notifications from file'\n",
      "\n",
      "user_names = set(user_messages['user'])\n",
      "min_day = raw_data['date_time'].min()\n",
      "max_day = raw_data['date_time'].max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded 10022 user messages and 75 WhatsApp notifications from file\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "split messages and write to files for ease of access."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concatenate_list(input_strings):\n",
      "    return reduce(lambda x,y : x + \".\" + y,input_strings)\n",
      "    \n",
      "\n",
      "def get_messages(user_name):    \n",
      "    this_user_messages = user_messages[user_messages['user']==user_name]\n",
      "    return concatenate_list(this_user_messages.message.values)\n",
      "\n",
      "def write_messages(messages, user_name):\n",
      "    output_file = open(os.path.join(__location__, 'nlp/user_messages/' + user_name + '.txt'),'w')\n",
      "    output_file.write(messages)\n",
      "    output_file.close()\n",
      "    \n",
      "for user_name in user_names:\n",
      "    messages = get_messages(user_name)\n",
      "    write_messages(messages, user_name)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter\n",
      "from nltk.stem.porter import *\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "import os\n",
      "import string\n",
      "import re\n",
      "\n",
      "stop_words = {'im','a','able','about','across','after','all','almost','also','am','among','an','and','any','are','as','at','be','because','been','but','by','can','cannot','could','dear','did','do','does','either','else','ever','every','for','from','get','got','had','has','have','he','her','hers','him','his','how','however','i','if','in','into','is','it','its','just','least','let','like','likely','may','me','might','most','must','my','neither','no','nor','not','of','off','often','on','only','or','other','our','own','rather','said','say','says','she','should','since','so','some','than','that','the','their','them','then','there','these','they','this','tis','to','too','twas','us','wants','was','we','were','what','when','where','which','while','who','whom','why','will','with','would','yet','you','your'}\n",
      "exclude = set(string.punctuation)\n",
      "\n",
      "print stopwords\n",
      "\n",
      "__location__ = os.path.realpath(\n",
      "    os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
      "\n",
      "\n",
      "corpusdir= os.path.join(__location__, 'nlp/user_messages/')\n",
      "newcorpus = PlaintextCorpusReader(corpusdir, '.*')\n",
      "\n",
      "token_dict = {}\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "def replace_punctiation_char(ch):\n",
      "    if ch in exclude:\n",
      "        return ' '\n",
      "    else :\n",
      "        return ch\n",
      "  \n",
      "def remove_websites(text):\n",
      "    return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)      \n",
      "\n",
      "def remove_punctiation(s):    \n",
      "    return ''.join(replace_punctiation_char(ch) for ch in s )\n",
      "    \n",
      "def remove_emojis(s):\n",
      "    return re.sub(r'EMOJI\\[[a-z\\d]*\\]', ' ', s)\n",
      "\n",
      "def remove_media(s):\n",
      "    return re.sub(r'MEDIA', ' ', s)\n",
      "    \n",
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(stemmer.stem(item))\n",
      "    return stemmed\n",
      "   \n",
      "def tokenize(text):\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "    stems = stem_tokens(tokens, stemmer)\n",
      "    return stems    \n",
      "    \n",
      "for infile in sorted(newcorpus.fileids()):\n",
      "    print infile # The fileids of each file.\n",
      "    fin =  newcorpus.open(infile)  # Opens the file.\n",
      "    text =  fin.read().strip() # Prints the content of the file\n",
      "    just_text = remove_websites(remove_media(remove_emojis(text)).lower())\n",
      "    no_punctuation = remove_punctiation(just_text)\n",
      "    token_dict[infile] = no_punctuation\n",
      "    \n",
      "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
      "tfs = tfidf.fit_transform(token_dict.values())\n",
      "\n",
      "print token_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}